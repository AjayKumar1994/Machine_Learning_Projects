{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSV file\n",
    "\n",
    "The input file is .csv and it contains 2 columns consisting of data and label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('round2_task_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': 'KG0OUA', 'data': 'Good morning', 'mess...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'L9DC9H', 'data': 'Location', 'message_...</td>\n",
       "      <td>whoAreYou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': 'ZQR6R5', 'data': 'hi', 'message_order'...</td>\n",
       "      <td>whoAreYou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': 'RH0M4E', 'data': 'Hi', 'message_order'...</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': 'WLVX8I', 'data': 'Hello', 'message_ord...</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data      label\n",
       "0  {'id': 'KG0OUA', 'data': 'Good morning', 'mess...   location\n",
       "1  {'id': 'L9DC9H', 'data': 'Location', 'message_...  whoAreYou\n",
       "2  {'id': 'ZQR6R5', 'data': 'hi', 'message_order'...  whoAreYou\n",
       "3  {'id': 'RH0M4E', 'data': 'Hi', 'message_order'...   greeting\n",
       "4  {'id': 'WLVX8I', 'data': 'Hello', 'message_ord...   greeting"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whoAreYou               627\n",
       "greeting                619\n",
       "notInterested           474\n",
       "dontMeetRequirements    160\n",
       "location                120\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': 'KG0OUA', 'data': 'Good morning', 'mess...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'id': '2YAC8M', 'data': 'Iska location kya ho...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'id': 'YBEI5V', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'id': '0C9Y5K', 'data': 'K', 'message_order':...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'id': 'N84XF2', 'data': 'How r u', 'message_o...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'id': 'GE2L4N', 'data': 'üì∑ GüòäüòäD Morning', 'me...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>{'id': 'U237AR', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>{'id': 'AX28KL', 'data': 'Hi', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>{'id': 'K7VCBE', 'data': 'I applied', 'message...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>{'id': 'ID97YE', 'data': 'No', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>{'id': 'GDHCBH', 'data': 'Hi', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>{'id': 'VHZG00', 'data': 'Hii', 'message_order...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>{'id': 'JZUIOW', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>{'id': 'JWOJ5F', 'data': 'Yas', 'message_order...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>{'id': 'X978V6', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>{'id': '4UZKPJ', 'data': 'Kahapar hai job', 'm...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>{'id': 'XCRIJW', 'data': 'Where s the office',...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>{'id': 'QYR8I7', 'data': 'Why I should deposit...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>{'id': 'PE42AW', 'data': 'hiii', 'message_orde...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>{'id': 'URQ329', 'data': 'K........tq so much ...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>{'id': 'K4C5K3', 'data': \"don't send any text ...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>{'id': '7PQIXJ', 'data': 'Ur location', 'messa...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>{'id': 'YPFW58', 'data': 'K', 'message_order':...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>{'id': 'XQ84T5', 'data': 'Which location', 'me...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>{'id': 'JS11UL', 'data': 'Snd the location', '...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>{'id': '5DHOLO', 'data': 'Kidar hey', 'message...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>{'id': '84TDV4', 'data': 'Okay', 'message_orde...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>{'id': 'NI60LW', 'data': 'Hi', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>{'id': 'MQ0JKV', 'data': 'Sir', 'message_order...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>{'id': 'DFUSH5', 'data': 'Bike nhi hai', 'mess...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>{'id': '86AD2F', 'data': 'Ok thank you', 'mess...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>{'id': '7U1ACE', 'data': 'Hi', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>{'id': 'YUBP3S', 'data': 'Which is nearest to ...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>{'id': '5SRH89', 'data': 'Applications fee nai...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>{'id': '1IV8BS', 'data': 'Hi friend', 'message...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>{'id': 'MZVG22', 'data': 'Hello', 'message_ord...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>{'id': 'TLKO9W', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>{'id': 'F2TL4M', 'data': 'Hellooo', 'message_o...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>{'id': 'RRZH2E', 'data': 'OK', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>{'id': 'APU6K0', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>{'id': '4V8AFG', 'data': 'Area', 'message_orde...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>{'id': 'MMGDLN', 'data': 'Hai', 'message_order...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>{'id': 'RK7OC5', 'data': 'Not insert', 'messag...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>{'id': 'BM3TLW', 'data': 'Kk', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>{'id': 'W5QBCO', 'data': \"Hi sir I'm Suchindra...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>{'id': '49VTA7', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>{'id': 'C0PYT6', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>{'id': '24ACRP', 'data': 'I have learning lice...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>{'id': 'B40NI9', 'data': 'Thnx', 'message_orde...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>{'id': 'LD3NM4', 'data': 'I have driving licen...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>{'id': 'PKBY3B', 'data': 'Where is interview',...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>{'id': 'LXFFKW', 'data': 'Mere paas bike nhi h...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>{'id': 'GAVZIO', 'data': 'Hi', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>{'id': 'CN4ALS', 'data': 'Edhi akada job', 'me...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>{'id': 'O5GI27', 'data': 'Hii', 'message_order...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>{'id': '3UNBKR', 'data': 'Kidar He', 'message_...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>{'id': '06UMAP', 'data': 'Hi', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>{'id': 'GYYOUQ', 'data': 'Hai', 'message_order...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>{'id': 'UDK5TF', 'data': 'Ok', 'message_order'...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>{'id': 'UC5H2C', 'data': 'Then sry', 'message_...</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data     label\n",
       "0     {'id': 'KG0OUA', 'data': 'Good morning', 'mess...  location\n",
       "27    {'id': '2YAC8M', 'data': 'Iska location kya ho...  location\n",
       "30    {'id': 'YBEI5V', 'data': 'Ok', 'message_order'...  location\n",
       "31    {'id': '0C9Y5K', 'data': 'K', 'message_order':...  location\n",
       "35    {'id': 'N84XF2', 'data': 'How r u', 'message_o...  location\n",
       "58    {'id': 'GE2L4N', 'data': 'üì∑ GüòäüòäD Morning', 'me...  location\n",
       "76    {'id': 'U237AR', 'data': 'Ok', 'message_order'...  location\n",
       "84    {'id': 'AX28KL', 'data': 'Hi', 'message_order'...  location\n",
       "85    {'id': 'K7VCBE', 'data': 'I applied', 'message...  location\n",
       "87    {'id': 'ID97YE', 'data': 'No', 'message_order'...  location\n",
       "108   {'id': 'GDHCBH', 'data': 'Hi', 'message_order'...  location\n",
       "133   {'id': 'VHZG00', 'data': 'Hii', 'message_order...  location\n",
       "162   {'id': 'JZUIOW', 'data': 'Ok', 'message_order'...  location\n",
       "164   {'id': 'JWOJ5F', 'data': 'Yas', 'message_order...  location\n",
       "177   {'id': 'X978V6', 'data': 'Ok', 'message_order'...  location\n",
       "192   {'id': '4UZKPJ', 'data': 'Kahapar hai job', 'm...  location\n",
       "227   {'id': 'XCRIJW', 'data': 'Where s the office',...  location\n",
       "256   {'id': 'QYR8I7', 'data': 'Why I should deposit...  location\n",
       "261   {'id': 'PE42AW', 'data': 'hiii', 'message_orde...  location\n",
       "262   {'id': 'URQ329', 'data': 'K........tq so much ...  location\n",
       "273   {'id': 'K4C5K3', 'data': \"don't send any text ...  location\n",
       "277   {'id': '7PQIXJ', 'data': 'Ur location', 'messa...  location\n",
       "294   {'id': 'YPFW58', 'data': 'K', 'message_order':...  location\n",
       "301   {'id': 'XQ84T5', 'data': 'Which location', 'me...  location\n",
       "317   {'id': 'JS11UL', 'data': 'Snd the location', '...  location\n",
       "369   {'id': '5DHOLO', 'data': 'Kidar hey', 'message...  location\n",
       "383   {'id': '84TDV4', 'data': 'Okay', 'message_orde...  location\n",
       "387   {'id': 'NI60LW', 'data': 'Hi', 'message_order'...  location\n",
       "400   {'id': 'MQ0JKV', 'data': 'Sir', 'message_order...  location\n",
       "403   {'id': 'DFUSH5', 'data': 'Bike nhi hai', 'mess...  location\n",
       "...                                                 ...       ...\n",
       "1570  {'id': '86AD2F', 'data': 'Ok thank you', 'mess...  location\n",
       "1582  {'id': '7U1ACE', 'data': 'Hi', 'message_order'...  location\n",
       "1594  {'id': 'YUBP3S', 'data': 'Which is nearest to ...  location\n",
       "1603  {'id': '5SRH89', 'data': 'Applications fee nai...  location\n",
       "1618  {'id': '1IV8BS', 'data': 'Hi friend', 'message...  location\n",
       "1638  {'id': 'MZVG22', 'data': 'Hello', 'message_ord...  location\n",
       "1641  {'id': 'TLKO9W', 'data': 'Ok', 'message_order'...  location\n",
       "1645  {'id': 'F2TL4M', 'data': 'Hellooo', 'message_o...  location\n",
       "1648  {'id': 'RRZH2E', 'data': 'OK', 'message_order'...  location\n",
       "1686  {'id': 'APU6K0', 'data': 'Ok', 'message_order'...  location\n",
       "1716  {'id': '4V8AFG', 'data': 'Area', 'message_orde...  location\n",
       "1765  {'id': 'MMGDLN', 'data': 'Hai', 'message_order...  location\n",
       "1773  {'id': 'RK7OC5', 'data': 'Not insert', 'messag...  location\n",
       "1789  {'id': 'BM3TLW', 'data': 'Kk', 'message_order'...  location\n",
       "1792  {'id': 'W5QBCO', 'data': \"Hi sir I'm Suchindra...  location\n",
       "1796  {'id': '49VTA7', 'data': 'Ok', 'message_order'...  location\n",
       "1802  {'id': 'C0PYT6', 'data': 'Ok', 'message_order'...  location\n",
       "1810  {'id': '24ACRP', 'data': 'I have learning lice...  location\n",
       "1872  {'id': 'B40NI9', 'data': 'Thnx', 'message_orde...  location\n",
       "1873  {'id': 'LD3NM4', 'data': 'I have driving licen...  location\n",
       "1880  {'id': 'PKBY3B', 'data': 'Where is interview',...  location\n",
       "1882  {'id': 'LXFFKW', 'data': 'Mere paas bike nhi h...  location\n",
       "1885  {'id': 'GAVZIO', 'data': 'Hi', 'message_order'...  location\n",
       "1888  {'id': 'CN4ALS', 'data': 'Edhi akada job', 'me...  location\n",
       "1900  {'id': 'O5GI27', 'data': 'Hii', 'message_order...  location\n",
       "1910  {'id': '3UNBKR', 'data': 'Kidar He', 'message_...  location\n",
       "1934  {'id': '06UMAP', 'data': 'Hi', 'message_order'...  location\n",
       "1945  {'id': 'GYYOUQ', 'data': 'Hai', 'message_order...  location\n",
       "1965  {'id': 'UDK5TF', 'data': 'Ok', 'message_order'...  location\n",
       "1974  {'id': 'UC5H2C', 'data': 'Then sry', 'message_...  location\n",
       "\n",
       "[120 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.label == 'location' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train[['data']]\n",
    "Y = train['label']\n",
    "np.shape(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting required data\n",
    "The data is in the form of dictionary but we are only intersted in the data key, since the message(chat) is stored as value for the data key. So, we extract the value from the 'data' key and store them as  list elements.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "\n",
    "X_lst = X.values.tolist()\n",
    "Y_lst = Y.values.tolist()\n",
    "datas = []\n",
    "for dt in X_lst:\n",
    "    x_l = ast.literal_eval(dt[0])\n",
    "    datas.append(x_l['data'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "In order to perform machine learning on text documents, we first need to turn these text content into numerical feature vectors that Scikit-Learn can use. The simplest and most intuitive way to do so, is the ‚Äúbags-of-words‚Äù representation which ignores structure and simply counts how often each word occurs. CountVectorizer allows us to use the bags-of-words approach, by converting a collection of text documents into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(datas)\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default configuration tokenizes the string, by extracting words of at least 2 letters or numbers, separated by word boundaries, it then converts everything to lowercase and builds a vocabulary using these tokens. We can get some of the vocabularies by using the get_feature_names method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0lz', 'bhr', 'drive', 'hr', 'lagana', 'nearest', 'prasent', 'ther', '‡∞¶‡∞Ø']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at those vocabularies, we can get a small sense of what they are about . By checking the length of get_feature_names, we can see that we‚Äôre working with 804 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the documents in datas to a document term matrix, which gives us the bags-of-word representation of datas. The result is stored in a SciPy sparse matrix, where each row corresponds to a document, and each column is a word from our training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 804)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_vectorized = vect.transform(datas)\n",
    "datas_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Logistic Regression\n",
    "\n",
    "Since, we have the data in vectorized format we can now apply Machine Learning algorithms, to figure out the accuracy of our model. Logistic Regression is one of the ML classification techniques, which can be described in simple terms - Data is fit into linear regression model, which then be acted upon by a logistic function predicting the target categorical dependent variable.Ref.(1)\n",
    "\n",
    "For fitting the model with Logistic Regression, I have used OnevsRest classifier. This strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. This strategy is used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.\n",
    "\n",
    "3 fold cross validation technique have been used where our training data is split into 3 parts. The first fold is treated as a validation set, and the method is fit on the remaining 2 folds. It is called K - fold cross validation where in this case k = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.5 ms, sys: 0 ns, total: 80.5 ms\n",
      "Wall time: 79.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(datas_vectorized, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Dumping\n",
    "Used cloud pickle to save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "ouf = open('model_LR.txt', 'wb')\n",
    "cloudpickle.dump(ovr, ouf)\n",
    "cloudpickle.dump(range(19), ouf)\n",
    "ouf.close(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 31.40%, std 0.13.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, datas_vectorized, Y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer\n",
    "This data is further processed by applying Tfidf Vectorizer, which helps us to give more weight-age to important words which less important words for the case study would be given more weights.\n",
    "\n",
    "Since, our code is based on counting the frequency of each word in the document, so if certain words like ‚Äòthe‚Äô, ‚Äòif‚Äô etc. which are present more frequently then words which are more important such as ‚Äòbuy‚Äô,‚Äôproduct‚Äô etc. , which gives us the context. Ref.(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation accuracy is 31.40 % which is quite low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer().fit(datas)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-grams is set  in the range of 1‚Äì2 which helps us to extract features for 1 and 2 grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2270"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range = (1,2)).fit(datas)\n",
    "datas_tfidf = vect.transform(datas)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.5 ms, sys: 3.62 ms, total: 58.1 ms\n",
      "Wall time: 59.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(datas_tfidf, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Dumping\n",
    "Used cloud pickle to save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "ouf = open('model_LR_tfidf.txt', 'wb')\n",
    "cloudpickle.dump(ovr, ouf)\n",
    "cloudpickle.dump(range(19), ouf)\n",
    "ouf.close(  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 32.00%, std 0.75.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, datas_tfidf, Y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I was expecting better results with tfidf and n-grams but unfortunately there was not much change in the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier for multinomial models\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. Ref.(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 30.90%, std 0.41.\n",
      "CPU times: user 76.5 ms, sys: 27.2 ms, total: 104 ms\n",
      "Wall time: 209 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = MultinomialNB()\n",
    "\n",
    "scores =  cross_val_score(model, datas_tfidf, Y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Dumping\n",
    "Used cloud pickle to save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "ouf = open('model_NB_tfidf.txt', 'wb')\n",
    "cloudpickle.dump(ovr, ouf)\n",
    "cloudpickle.dump(range(19), ouf)\n",
    "ouf.close(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier\n",
    "\n",
    "The maximal margin classifier is a very natural way to perform classification, is a separating hyperplane exists. However existence of such a hyperplane may not be guaranteed, or even if it exists, the data is noisy so that maximal margin classifier provides a poor solution. In such cases, the concept can be extended where a hyperplane exists which almost separates the classes, using what is known as a soft margin. \n",
    "\n",
    "The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier, where a small proportion of the training sample is allowed to cross the margins, or even the separating hyperplane. Rather than looking for the largest possible margin so that every observation is on the correct side of the margin, thereby making the margins very narrow or non-existent, some observations are allowed to be on the incorrect side of the margins. \n",
    "\n",
    "The margin is soft as a small number of observations violate the margin. The softness is controlled by slack variables which control the position of the observations relative to the margins and separating hyperplane. The support vector classifier maximizes a soft margin. Ref.(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 31.45%, std 0.92.\n",
      "CPU times: user 81.1 ms, sys: 51.6 ms, total: 133 ms\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, datas_tfidf, Y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Dumping\n",
    "Used cloud pickle to save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "ouf = open('model_SVC_tfidf.txt', 'wb')\n",
    "cloudpickle.dump(ovr, ouf)\n",
    "cloudpickle.dump(range(19), ouf)\n",
    "ouf.close(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: By using various ML techniques the accuracy is low around 30-32 %. Although, the labeling of the training data could be one issue, but nevertheless further data preprocessing techniques such as resampling the data for balancing or lemmatization could lead to better results. Also, we could use word2vec model, to check if the accuracy improves any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "2. https://medium.com/@josephroy/when-i-decided-to-work-on-sentiment-analysis-amazon-fine-food-review-kaggle-project-was-quite-3575721a8849\n",
    "3. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "4. https://onlinecourses.science.psu.edu/stat857/node/241/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
